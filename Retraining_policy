# step1_assess_highly_affected_runtime.py
# --------------------------------------
# Usage (pseudo-code):
#
#   import labornet as lbn
#   from step1_assess_highly_affected_runtime import assess_highly_affected
#
#   # --- set / load your calibrated inputs here ----------------------------
#   t_simulation   = 600
#   t_shock        = 100
#   τ              = 3
#   δ_u, δ_v       = 0.016, 0.012
#   γ_u, γ_v       = 10*δ_u, 10*δ_u
#   k, sigmoid_hl  = lbn.calibrate_sigmoid(30*52/6.75)   # as in paper
#   employment_0   = ...   # vector length 464
#   unemployment_0 = δ_u * employment_0
#   vacancies_0    = δ_v * employment_0
#   D_0            = employment_0 + unemployment_0
#   D_f            = lbn.labor_restructure(D_0, p)       # p = automation probs
#   A_omn_hetero   = np.genfromtxt("data/occupational_mobility_network_heteroloops.csv",
#                                  delimiter=',')
#   # -----------------------------------------------------------------------
#
#   df = assess_highly_affected(t_simulation, t_shock, A_omn_hetero, τ,
#                               δ_u, δ_v, γ_u, γ_v,
#                               employment_0, unemployment_0, vacancies_0,
#                               D_0, D_f, k, sigmoid_hl)
#   print(df.head(20))          # or df.to_csv("highly_affected.csv", index=False)

import numpy as np
import pandas as pd
from pathlib import Path
import labornet as lbn


def assess_highly_affected(t_simulation, t_shock, A_omn_hetero, τ,
                           δ_u, δ_v, γ_u, γ_v,
                           employment_0, unemployment_0, vacancies_0,
                           D_0, D_f, k, sigmoid_half_life):
    """
    Run the heterogenous-loop OMN once and return a DataFrame with:

        occupation | name | ΔU_peak | U_baseline | employment_pre | automation_prob | severity_score
    """
    # ------------------------------------------------------------------ #
    # 1. Run the numerical solution (rows = timesteps, columns = occs). #
    # ------------------------------------------------------------------ #
    U, V, E, U_all, D = lbn.run_numerical_solution(
        lbn.fire_and_hire_workers, t_simulation,
        δ_u, δ_v, γ_u, γ_v,
        employment_0, unemployment_0, vacancies_0,
        lbn.target_demand_automation, D_0, D_f,
        t_shock, k, sigmoid_half_life,
        lbn.matching_probability, A_omn_hetero, τ
    )

    # -------------------------------------------------------------- #
    # 2. Baseline & spike in unemployment rate (per occupation).    #
    # -------------------------------------------------------------- #
    U_rate      = U / (U + E)                 # unemployment-rate matrix
    U_baseline  = U_rate[t_shock - 1, :]      # vector length n_occ
    ΔU_peak     = (U_rate[t_shock:, :] - U_baseline).max(axis=0)

    # employment right before the shock
    employment_pre = E[t_shock - 1, :]

    # -------------------------------------------------------------- #
    # 3. Static occupation attributes (names, automation prob).      #
    # -------------------------------------------------------------- #
    df_labs   = pd.read_csv(Path("data") / "ipums_variables.csv")
    names     = df_labs["label"].values                       # names/index match model order:contentReference[oaicite:0]{index=0}
    auto_prob = df_labs["auto_prob_average"].values

    # -------------------------------------------------------------- #
    # 4. Composite severity score (same formula we agreed on).       #
    # -------------------------------------------------------------- #
    severity = (ΔU_peak / np.maximum(U_baseline, 1e-9)) * employment_pre * auto_prob

    # -------------------------------------------------------------- #
    # 5. Bundle everything into a tidy DataFrame.                    #
    # -------------------------------------------------------------- #
    df_out = (pd.DataFrame({
                "occupation"      : np.arange(A_omn_hetero.shape[0]),
                "name"            : names,
                "ΔU_peak"         : ΔU_peak,
                "U_baseline"      : U_baseline,
                "employment_pre"  : employment_pre,
                "automation_prob" : auto_prob,
                "severity_score"  : severity
              })
              .sort_values("severity_score", ascending=False)
              .reset_index(drop=True))

    return df_out


# candidate_retraining_edges_nx.py
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path

def candidate_retraining_edges_nx(V, E, A, t_shock, source_id,
                                  top_k=10,
                                  small_threshold=0.01,
                                  names_path="data/ipums_variables.csv"):
    """
    Parameters
    ----------
    V, E : ndarray  (timesteps × occupations)
        Vacancy and employment time-series from run_numerical_solution.
    A : ndarray  (occupations × occupations)
        Row-stochastic mobility matrix; A[i,j] is P(move i→j).
    t_shock : int
        Index where the automation shock begins.
    source_id : int
        Occupation that needs new pathways.
    top_k : int
        How many unlinked candidates to return.
    small_threshold : float
        Upper bound on an edge weight for it to count as “very small”.
    names_path : str
        CSV with columns “label” and “auto_prob_average”.
        Default path matches the repository layout.                            # :contentReference[oaicite:0]{index=0}

    Returns
    -------
    df_unlinked, df_weak : pandas.DataFrame
    """
    n_occ = A.shape[0]

    # ---------------------------------------------------------
    # 1. Vacancy-rate spike  ΔV_peak  (baseline = t_shock-1)
    # ---------------------------------------------------------
    V_rate  = V / (V + E)
    V_base  = V_rate[t_shock - 1, :]
    ΔV_peak = (V_rate[t_shock:, :] - V_base).max(axis=0)

    # ---------------------------------------------------------
    # 2. Build a directed graph   cost = −log(probability)
    #    High transition-probability → short path
    # ---------------------------------------------------------
    G = nx.DiGraph()
    rows, cols = np.nonzero(A)
    for i, j in zip(rows, cols):
        prob = A[i, j]
        cost = -np.log(prob)
        G.add_edge(i, j, weight=cost, prob=prob)

    # shortest-path distances from the source occupation
    dist_dict = nx.single_source_dijkstra_path_length(G, source=source_id,
                                                      weight="weight")

    # ---------------------------------------------------------
    # 3. Occupation meta-data (names & automation prob)
    # ---------------------------------------------------------
    df_labs   = pd.read_csv(Path(names_path))
    names     = df_labs["label"].values
    auto_prob = df_labs["auto_prob_average"].values

    # ---------------------------------------------------------
    # 4. Masks for unlinked and weakly linked candidates
    # ---------------------------------------------------------
    source_edges = A[source_id, :]          # weights of direct links i→j
    is_neighbour = source_edges > 0
    idx_all      = np.arange(n_occ)

    # (a)  not directly connected  &  reachable
    mask_unlinked = (~is_neighbour) & (idx_all != source_id) \
                    & np.isin(idx_all, list(dist_dict.keys()))
    targets_unlinked = idx_all[mask_unlinked]

    # (b)  directly connected but weight < threshold
    mask_weak = is_neighbour & (source_edges < small_threshold)
    targets_weak = idx_all[mask_weak]

    # ---------------------------------------------------------
    # 5. Assemble DataFrames
    # ---------------------------------------------------------
    df_unlinked = (pd.DataFrame({
                    "target_id"      : targets_unlinked,
                    "name"           : names[targets_unlinked],
                    "dist"           : [dist_dict[i] for i in targets_unlinked],
                    "ΔV_peak"        : ΔV_peak[targets_unlinked],
                    "V_baseline"     : V_base[targets_unlinked],
                    "automation_prob": auto_prob[targets_unlinked]
                  })
                  .sort_values(["dist", "ΔV_peak"], ascending=[True, False])
                  .head(top_k)
                  .reset_index(drop=True))

    df_weak = (pd.DataFrame({
                 "target_id"      : targets_weak,
                 "name"           : names[targets_weak],
                 "edge_weight"    : source_edges[targets_weak],
                 "ΔV_peak"        : ΔV_peak[targets_weak],
                 "V_baseline"     : V_base[targets_weak],
                 "automation_prob": auto_prob[targets_weak]
               })
               .sort_values(["edge_weight", "ΔV_peak"],
                            ascending=[True, False])
               .reset_index(drop=True))

    return df_unlinked, df_weak

# inject_edges.py
# ----------------------------------------------------------
# 1) inject_edges()        – rewrites ONE row of A in-place
# 2) run_with_injection()  – convenience wrapper that
#                            • runs the model until t_shock
#                            • injects the new links
#                            • continues the simulation
#                            • stitches the two runs together
#
# You can of course skip (2) and just call inject_edges() if
# you handle the time split yourself.
# ----------------------------------------------------------

import numpy as np
import labornet as lbn   # import your local labornet

# ---------- 1. edge-injector ------------------------------------------- #
def inject_edges(A, source_id, target_ids, w_total=0.10):
    """
    Add/strengthen links from `source_id` to every occ in `target_ids`
    so that together they receive `w_total` probability mass.
    Existing outgoing probs from source are scaled by (1 − w_total).
    The modified matrix is returned; `A` itself is *not* mutated.

    Parameters
    ----------
    A : ndarray [n_occ × n_occ]   – row-stochastic mobility matrix.
    source_id : int              – occupation to retrain OUT of.
    target_ids : Sequence[int]   – occupations to receive new workers.
    w_total : float              – total probability mass for new links
                                   (e.g. 0.10 → 10 %).

    Returns
    -------
    A_new : ndarray  (copy of A with injected edges)
    """
    A_new = A.copy()
    if not target_ids:
        return A_new

    # scale the whole row down so we free up w_total mass
    A_new[source_id, :] *= (1.0 - w_total)

    # distribute the freed mass equally across chosen targets
    add_w = w_total / len(target_ids)
    for j in target_ids:
        A_new[source_id, j] += add_w

    # robust row re-normalisation (floats can drift)
    A_new[source_id, :] /= A_new[source_id, :].sum()

    return A_new


# ---------- 2. two-phase run ------------------------------------------ #
def run_with_injection(t_simulation, t_shock,
                       δ_u, δ_v, γ_u, γ_v,
                       employment_0, unemployment_0, vacancies_0,
                       D_0, D_f, k, sigmoid_half_life,
                       A_orig, τ,
                       source_id, target_ids, w_total=0.10):
    """
    Thin convenience wrapper: run the baseline up to `t_shock−1`,
    inject new edges, then finish the horizon and *concatenate*
    the two result blocks so the output shape matches the usual
    (t_simulation × n_occ) expectation.

    Returns
    -------
    U, V, E, U_all, D : each ndarray with full length t_simulation
    """
    # ---- phase 1: pre-shock (A_orig) ----
    U1, V1, E1, U_all1, D1 = lbn.run_numerical_solution(
        lbn.fire_and_hire_workers, t_shock,
        δ_u, δ_v, γ_u, γ_v,
        employment_0, unemployment_0, vacancies_0,
        lbn.target_demand_automation, D_0, D_f,
        t_shock, k, sigmoid_half_life,
        lbn.matching_probability, A_orig, τ
    )

    # extract end-of-phase1 state to warm-start phase 2
    emp0  = E1[-1, :].copy()
    unemp0= U1[-1, :].copy()
    vac0  = V1[-1, :].copy()
    D0_phase2 = D1[-1, :].copy()

    # ---- inject edges & build new matrix ----
    A_mod = inject_edges(A_orig, source_id, target_ids, w_total)

    # ---- phase 2: remainder using A_mod ----
    steps_left = t_simulation - t_shock
    U2, V2, E2, U_all2, D2 = lbn.run_numerical_solution(
        lbn.fire_and_hire_workers, steps_left,
        δ_u, δ_v, γ_u, γ_v,
        emp0, unemp0, vac0,
        lbn.target_demand_automation, D0_phase2, D_f,
        t_shock, k, sigmoid_half_life,
        lbn.matching_probability, A_mod, τ
    )

    # ---- stitch (discard duplicate row at cut-over) ----
    U       = np.vstack([U1,       U2[1:]])
    V       = np.vstack([V1,       V2[1:]])
    E       = np.vstack([E1,       E2[1:]])
    U_all   = np.vstack([U_all1,   U_all2[1:]])
    D       = np.vstack([D1,       D2[1:]])

    return U, V, E, U_all, D
