# candidate_retraining_edges.py
import numpy as np
import pandas as pd
from pathlib import Path
from scipy.sparse.csgraph import shortest_path

def candidate_retraining_edges(V, E, A, t_shock, source_id,
                               top_k=10,
                               names_path="data/ipums_variables.csv"):
    """
    Identify target occupations whose vacancy-rate spikes most during the
    automation shock and are close (but not already connected) to `source_id`.

    Parameters
    ----------
    V, E : np.ndarray
        Vacancy and employment matrices (rows = timesteps, cols = occupations);
        this orientation matches the arrays saved in `RunNumericalShock.py` :contentReference[oaicite:0]{index=0}.
    A : np.ndarray
        Row-stochastic occupational-mobility adjacency matrix.
    t_shock : int
        Index where the shock starts.
    source_id : int
        Index of the high-unemployment occupation you want to help.
    top_k : int
        How many candidate targets to return.
    names_path : str
        CSV with columns `label` (names) and `auto_prob_average`.

    Returns
    -------
    pandas.DataFrame
        Columns: [target_id, name, dist, ΔV_peak, V_baseline, automation_prob]
    """
    # ---------- 1. vacancy-rate spike ------------------------------------ #
    V_rate   = V / (V + E)
    V_base   = V_rate[t_shock - 1, :]                 # baseline vacancy rate
    ΔV_peak  = (V_rate[t_shock:, :] - V_base).max(axis=0)

    # ---------- 2. weighted shortest paths from source ------------------- #
    # cost = −log(weight); unreachable edges → ∞
    cost_mat = np.where(A > 0, -np.log(A), np.inf)
    dists    = shortest_path(cost_mat, directed=True, indices=source_id)

    # ---------- 3. mask: drop self & existing neighbours ----------------- #
    is_neighbour = A[source_id, :] > 0
    mask = (~is_neighbour) & np.isfinite(dists) & (np.arange(A.shape[0]) != source_id)

    # ---------- 4. human-readable labels --------------------------------- #
    df_labs  = pd.read_csv(Path(names_path))
    names    = df_labs["label"].values
    auto_prob= df_labs["auto_prob_average"].values

    # ---------- 5. assemble & sort --------------------------------------- #
    df = (pd.DataFrame({
            "target_id"      : np.arange(A.shape[0])[mask],
            "name"           : names[mask],
            "dist"           : dists[mask],
            "ΔV_peak"        : ΔV_peak[mask],
            "V_baseline"     : V_base[mask],
            "automation_prob": auto_prob[mask]
          })
          .sort_values(["dist", "ΔV_peak"], ascending=[True, False])
          .head(top_k)
          .reset_index(drop=True))

    return df

# candidate_retraining_edges_nx.py
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path

def candidate_retraining_edges_nx(V, E, A, t_shock, source_id,
                                  top_k=10,
                                  small_threshold=0.01,
                                  names_path="data/ipums_variables.csv"):
    """
    Parameters
    ----------
    V, E : ndarray  (timesteps × occupations)
        Vacancy and employment time-series from run_numerical_solution.
    A : ndarray  (occupations × occupations)
        Row-stochastic mobility matrix; A[i,j] is P(move i→j).
    t_shock : int
        Index where the automation shock begins.
    source_id : int
        Occupation that needs new pathways.
    top_k : int
        How many unlinked candidates to return.
    small_threshold : float
        Upper bound on an edge weight for it to count as “very small”.
    names_path : str
        CSV with columns “label” and “auto_prob_average”.
        Default path matches the repository layout.                            # :contentReference[oaicite:0]{index=0}

    Returns
    -------
    df_unlinked, df_weak : pandas.DataFrame
    """
    n_occ = A.shape[0]

    # ---------------------------------------------------------
    # 1. Vacancy-rate spike  ΔV_peak  (baseline = t_shock-1)
    # ---------------------------------------------------------
    V_rate  = V / (V + E)
    V_base  = V_rate[t_shock - 1, :]
    ΔV_peak = (V_rate[t_shock:, :] - V_base).max(axis=0)

    # ---------------------------------------------------------
    # 2. Build a directed graph   cost = −log(probability)
    #    High transition-probability → short path
    # ---------------------------------------------------------
    G = nx.DiGraph()
    rows, cols = np.nonzero(A)
    for i, j in zip(rows, cols):
        prob = A[i, j]
        cost = -np.log(prob)
        G.add_edge(i, j, weight=cost, prob=prob)

    # shortest-path distances from the source occupation
    dist_dict = nx.single_source_dijkstra_path_length(G, source=source_id,
                                                      weight="weight")

    # ---------------------------------------------------------
    # 3. Occupation meta-data (names & automation prob)
    # ---------------------------------------------------------
    df_labs   = pd.read_csv(Path(names_path))
    names     = df_labs["label"].values
    auto_prob = df_labs["auto_prob_average"].values

    # ---------------------------------------------------------
    # 4. Masks for unlinked and weakly linked candidates
    # ---------------------------------------------------------
    source_edges = A[source_id, :]          # weights of direct links i→j
    is_neighbour = source_edges > 0
    idx_all      = np.arange(n_occ)

    # (a)  not directly connected  &  reachable
    mask_unlinked = (~is_neighbour) & (idx_all != source_id) \
                    & np.isin(idx_all, list(dist_dict.keys()))
    targets_unlinked = idx_all[mask_unlinked]

    # (b)  directly connected but weight < threshold
    mask_weak = is_neighbour & (source_edges < small_threshold)
    targets_weak = idx_all[mask_weak]

    # ---------------------------------------------------------
    # 5. Assemble DataFrames
    # ---------------------------------------------------------
    df_unlinked = (pd.DataFrame({
                    "target_id"      : targets_unlinked,
                    "name"           : names[targets_unlinked],
                    "dist"           : [dist_dict[i] for i in targets_unlinked],
                    "ΔV_peak"        : ΔV_peak[targets_unlinked],
                    "V_baseline"     : V_base[targets_unlinked],
                    "automation_prob": auto_prob[targets_unlinked]
                  })
                  .sort_values(["dist", "ΔV_peak"], ascending=[True, False])
                  .head(top_k)
                  .reset_index(drop=True))

    df_weak = (pd.DataFrame({
                 "target_id"      : targets_weak,
                 "name"           : names[targets_weak],
                 "edge_weight"    : source_edges[targets_weak],
                 "ΔV_peak"        : ΔV_peak[targets_weak],
                 "V_baseline"     : V_base[targets_weak],
                 "automation_prob": auto_prob[targets_weak]
               })
               .sort_values(["edge_weight", "ΔV_peak"],
                            ascending=[True, False])
               .reset_index(drop=True))

    return df_unlinked, df_weak

# inject_edges.py
# ----------------------------------------------------------
# 1) inject_edges()        – rewrites ONE row of A in-place
# 2) run_with_injection()  – convenience wrapper that
#                            • runs the model until t_shock
#                            • injects the new links
#                            • continues the simulation
#                            • stitches the two runs together
#
# You can of course skip (2) and just call inject_edges() if
# you handle the time split yourself.
# ----------------------------------------------------------

import numpy as np
import labornet as lbn   # import your local labornet

# ---------- 1. edge-injector ------------------------------------------- #
def inject_edges(A, source_id, target_ids, w_total=0.10):
    """
    Add/strengthen links from `source_id` to every occ in `target_ids`
    so that together they receive `w_total` probability mass.
    Existing outgoing probs from source are scaled by (1 − w_total).
    The modified matrix is returned; `A` itself is *not* mutated.

    Parameters
    ----------
    A : ndarray [n_occ × n_occ]   – row-stochastic mobility matrix.
    source_id : int              – occupation to retrain OUT of.
    target_ids : Sequence[int]   – occupations to receive new workers.
    w_total : float              – total probability mass for new links
                                   (e.g. 0.10 → 10 %).

    Returns
    -------
    A_new : ndarray  (copy of A with injected edges)
    """
    A_new = A.copy()
    if not target_ids:
        return A_new

    # scale the whole row down so we free up w_total mass
    A_new[source_id, :] *= (1.0 - w_total)

    # distribute the freed mass equally across chosen targets
    add_w = w_total / len(target_ids)
    for j in target_ids:
        A_new[source_id, j] += add_w

    # robust row re-normalisation (floats can drift)
    A_new[source_id, :] /= A_new[source_id, :].sum()

    return A_new


# ---------- 2. two-phase run ------------------------------------------ #
def run_with_injection(t_simulation, t_shock,
                       δ_u, δ_v, γ_u, γ_v,
                       employment_0, unemployment_0, vacancies_0,
                       D_0, D_f, k, sigmoid_half_life,
                       A_orig, τ,
                       source_id, target_ids, w_total=0.10):
    """
    Thin convenience wrapper: run the baseline up to `t_shock−1`,
    inject new edges, then finish the horizon and *concatenate*
    the two result blocks so the output shape matches the usual
    (t_simulation × n_occ) expectation.

    Returns
    -------
    U, V, E, U_all, D : each ndarray with full length t_simulation
    """
    # ---- phase 1: pre-shock (A_orig) ----
    U1, V1, E1, U_all1, D1 = lbn.run_numerical_solution(
        lbn.fire_and_hire_workers, t_shock,
        δ_u, δ_v, γ_u, γ_v,
        employment_0, unemployment_0, vacancies_0,
        lbn.target_demand_automation, D_0, D_f,
        t_shock, k, sigmoid_half_life,
        lbn.matching_probability, A_orig, τ
    )

    # extract end-of-phase1 state to warm-start phase 2
    emp0  = E1[-1, :].copy()
    unemp0= U1[-1, :].copy()
    vac0  = V1[-1, :].copy()
    D0_phase2 = D1[-1, :].copy()

    # ---- inject edges & build new matrix ----
    A_mod = inject_edges(A_orig, source_id, target_ids, w_total)

    # ---- phase 2: remainder using A_mod ----
    steps_left = t_simulation - t_shock
    U2, V2, E2, U_all2, D2 = lbn.run_numerical_solution(
        lbn.fire_and_hire_workers, steps_left,
        δ_u, δ_v, γ_u, γ_v,
        emp0, unemp0, vac0,
        lbn.target_demand_automation, D0_phase2, D_f,
        t_shock, k, sigmoid_half_life,
        lbn.matching_probability, A_mod, τ
    )

    # ---- stitch (discard duplicate row at cut-over) ----
    U       = np.vstack([U1,       U2[1:]])
    V       = np.vstack([V1,       V2[1:]])
    E       = np.vstack([E1,       E2[1:]])
    U_all   = np.vstack([U_all1,   U_all2[1:]])
    D       = np.vstack([D1,       D2[1:]])

    return U, V, E, U_all, D
